{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Glossary\n",
    "\n",
    "## Statistics\n",
    "\n",
    "### Foundation\n",
    "\n",
    "\n",
    "* Chance - proportion of times event should happen over repeated trials\n",
    "* P(A) - proportion of times event A happens in n trials\n",
    "* Law of Large Numbers - as n, the number of trials, grows larger and approaches infinity,  P(A) approaches \n",
    "* Outcome space - all possible results of a trial, P(outcome space) = 1  for any trial (i.e. the probability of a quarter flipping either heads or tails up is 100%).  The probability of an impossible result is 0.\n",
    "* Complement of an event A ($A_c$ or $\\overline{\\mbox{A}}$, depending on notation) - all events not including A, with total probability of $1 - P(A)$\n",
    "* Union of events A and B ($A \\cup B$) - any event that includes event A, event B, or both A and B.\n",
    "* Intersection of events A and B ($A \\cap B$) - any event that includes both A and B\n",
    "* Subset: An event A is a subset of event B if the A is within B\n",
    "* Partition - an event can be partitioned into non-intersecting sub-events (event A partitioned into sub-events $A_1, A_2...A_n)$.  If the probability of the intersection of sub-events is 0 (no sub-events overlap) then $A_1, A_2...A_n)$ form a partition.\n",
    "    * $A_1, A_2, A_3... A_n$ form a partition of $A$ if $A = A_1 \\cup A_2 \\cup A_3 ...\\cup A_n$ and $A_i \\cap A_j$ for all $i \\neq j, i,j \\leq n$ \n",
    "    \n",
    "    \n",
    "    \n",
    "* Rules of Probability:\n",
    "    * Probability of any event within the outcome space is at least 0.  $(P(A) ≥ 0)$\n",
    "    * If the sub-events $A_1, A_2...A_n$ form a partition of the event $A$, then $P(A) = P(A_1) + P(A_2) + ... + P(A_n)$\n",
    "    * Probability of the outcome space is 1; the sum of the probability of all possible and impossible events within the outcome space is 1.\n",
    "* Probability Space - defines the universe of a statistical model using 3 parts:\n",
    "    * Sample space -  the collection of all possible outcomes\n",
    "    * Event space - the collection of all possible sets of possible outcomes.\n",
    "    * Probability measure - a function that maps each event to a probability within the $[0,1]$ interval\n",
    "* Independence - event A is independent of event B if the occurrence of one does not affect the other\n",
    "* Multiplicative Law of Probability - if A and B are independent events, then $P(A \\cap B) = P(A)*P(B)$\n",
    "* Addition Law of Probability - if A and B are independent events, then $P(A \\cup B) = P(A)+P(B)-P(A \\cap B)$\n",
    "* Conditional Probability ($P(A|B)$) - reframing the probability of an event A given information about the occurrence of some event B\n",
    "* Law of Total Probability - for a partition $A_1, A_2...A_3$ of the sample space and for event $B$ of the sample space, $P(B) = \\sum_i P(B \\cap A_i)$\n",
    "    * If each partition $A_i$ has a positive probability (i.e. the subevent $A_i$ has a non-zero probability of existing), then by the Multiplicative Law of Probability, $P(B) = \\sum_i P(B|A_i)P(A_i)$\n",
    "* Bayes' Rule:\n",
    "    * $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "    * Alternatively, $P(B|A)= \\frac{P(A|B)P(B)}{\\sum_i P(A|B_i)P(B_i)}$\n",
    "\n",
    "\n",
    "* Random Variable - a variable without a fixed value.  Instead, a random variable describes any number of potential outcomes that may come from a random phenomenon.\n",
    "* Indicator random variable - a binary random variable used to describe failure/success (takes on value 0 or 1).  Often used in problems to simplify calculations.\n",
    " \n",
    "* Distribution - a function that divides the probability of outcome space into subsets in such a way that satisfies the rules of probability. \n",
    "    * For example, the distribution of a coin toss is $P(Heads) = 0.5, P(Tails = 0.5)$.\n",
    "    * Discrete distributions - random variables take on integer values (i.e. dice rolls, number of tickets bought in an hour, etc.)\n",
    "    * Continuous distributions - random variables take on continuous (decimal/real) values (i.e. time, distance, etc.)\n",
    "* Probability Mass Function/Probability Density Function (PMF/PDF) - a function that describes the probability of a random variable taking on a certain value (PMF/PDF: $P(X=x) = f(x)$)\n",
    "    * Probability Mass Function - used for discrete distributions\n",
    "    * Probability Density Function - used for continuous distributions.  A little more complicated than PMFs, since the absolute probability of a random value equaling an exact value is 0 due to the issue of preciseness (0 vs 0.00000000001).  Instead, the PDF describes a relative probability of a random value being within a certain interval containing that exact value.\n",
    "* Cumulative Density Function (CMF/CDF) - a function that describes the probability of a random variable being less than or equal to a certain value(PMF/PDF: $P(X<x) = F(x)$)\n",
    "    * For discrete distributions, the CDF can be defined as $P(X<x) = F(x) = \\sum_{x_i<x} f(X=x_i)$.  Essentially, we are adding the all the probabilities of X taking on all the values that are less than x.\n",
    "    * For continuous distributions, the CDF can be defined as  $P(X<x) = F(x) = \\int_{-\\infty}^x f(X=x_i)dx$.  Essentially, we are integrating to find the area under the curve up to the point of x.\n",
    "\n",
    "    \n",
    "* Joint Distributions\n",
    "* Marginal Distributions\n",
    "* Conditional Distribution\n",
    "\n",
    "\n",
    "* Expected Value - essentially a weighted sum of all outcomes by their probabilities\n",
    "    * For discrete distributions, $E(X) = \\sum P(x)x$\n",
    "    * For continuous distributions, $E(X) = \\int_{-\\infty}^{\\infty} P(x)x dx$\n",
    "    * Linearity of expectations: expected value of the sum of random variables is equal to the sum of their individual expected values, regardless of whether they are independent\n",
    "* Law of the Unthinking Statistician (LOTUS) - used to calculate the expectation of a function of a random variable.\n",
    "    * LOTUS (discrete): $E(g(X)) = \\sum g(x)f(x)$, where $f(x)$ is the PMF \n",
    "    * LOTUS(continuous): $E(g(X)) =\\int_{-\\infty}^{\\infty} f(x)g(x) dx$, where $f(x)$ is the PDF \n",
    "    \n",
    "* Variance\n",
    "    * $Var(X) = E[(X-\\mu)^2]$\n",
    "    * $Var(X) = Cov(X,X)$\n",
    "    * Expansion: $Var(X) = E[(X-E[X])^2]$\n",
    "        * $=E[(X-E[X])^2]$\n",
    "        * $=E[X^2 - 2XE[X]+E[X]^2]$\n",
    "        * $=E[X^2] - 2E[X]E[X]+E[X]^2$\n",
    "        * $=E[X^2] - 2E[X]^2 +E[X]^2$\n",
    "        * $=E[X^2]-E[X]^2$\n",
    "    * For the discrete case: \n",
    "        * $Var(X) = \\sum^n_{i=1} p_i (x_i - \\mu)^2$, or $Var(X) = \\sum^n_{i=1} p_i (x_i - E[X])^2$\n",
    "            * $\\mu = E[X] = \\sum^n_{i=1}p_i x_i$\n",
    "    * For the continuous case:\n",
    "        * $Var(X) = \\sigma^2 = \\int_{-\\infty}^{\\infty} (x-\\mu)^2f(x)dx$\n",
    "        * $ = \\int_{-\\infty}^{\\infty} x^2f(x)dx - 2u \\int_{-\\infty}^{\\infty} xf(x)dx + \\mu^2 \\int_{-\\infty}^{\\infty} f(x)dx$\n",
    "            \n",
    "    \n",
    "* Covariance\n",
    "    * $cov(X,Y) = E[(X-E[X])(Y-E[Y])]$\n",
    "    * We can use the linearity of expectation to simplify this:\n",
    "        * $cov(X,Y) = E[(X-E[X])(Y-E[Y])]$\n",
    "        * $ = E[XY - XE[Y] - E[X]Y + E[X]E[Y]]$\n",
    "        * $ = E[XY] - E[X]E[Y] - E[X]E[Y] + E[X]E[Y]$\n",
    "        * $ = E[XY] - E[X]E[Y]$\n",
    "    * Measure of the joint variability of two random variables\n",
    "        * Positive covariance = increasing X matches increasing Y\n",
    "        * Negative covariance = increasing X matches decreasing Y\n",
    "* Correlation\n",
    "    * Standardized form of covariance\n",
    "    * $corr(X,Y) = \\rho_{X,Y} = \\frac{cov(X,Y)}{\\sigma_X \\sigma_Y}$\n",
    "        * where $sigma$ is standard deviation \n",
    "        * using the definition of $cov(X,Y)$, we can rewrite correlation as:\n",
    "        * $corr(X,Y) = \\rho_{X,Y} = \\frac{E[(X-\\mu_X)(Y-\\mu_Y)]}{\\sigma_X \\sigma_Y}$\n",
    "    * Correlation of 0 = no discernable pattern\n",
    "    * Correlation of 1 = increasing X linearly and exactly matches increasing Y (X is a function of Y)\n",
    "    * Correlation of -1 = increasing X linearly and exactly matches decreasing Y (X is a function of Y)\n",
    "    * Else, there is noise\n",
    "    \n",
    "\n",
    "\n",
    "* Markov's Inequality\n",
    "    * $P(X \\geq a) \\leq \\frac{E(X)}{a}$\n",
    "        * When $a = \\tilde{a}*E(X)$ and $a > 0$: $P(X \\geq \\tilde{a}*E(X)) \\leq \\frac{1}{a}$\n",
    "    * Deals with edge cases, not non-negative\n",
    "* Chebyshev's Inequality\n",
    "    * For an integrable random variable X with finite $E(X) = \\mu$ and nonzero $Var(X) = \\sigma^2$: $P(|X-\\mu| \\geq k \\sigma) \\leq \\frac{1}{k^2}$\n",
    "    * Only useful for $k>1$ (all probabilities are $\\geq 1$ when $k \\leq 1$ as $\\frac{1}{k^2} /geq 1$)\n",
    "    * Chebyshev: sets bounds on both sides, allows better centering\n",
    "    * Can be applied to completely arbitrary distributions, so not super accurate/helpful if more details are known\n",
    "* Central Limit Theorem\n",
    "* Law of Averages\n",
    "\n",
    "\n",
    "* Craps Principle - for independent trials each with probability $p$ of success:\n",
    "    1. Find event A of your choosing with $P(A) = \\frac{1}{2}$\n",
    "    2. Wait until Success + Failure or Failure + Success; if Success + Success or Failure + Failure, try again\n",
    "    3. Find event B with p of your choosing\n",
    "    \n",
    "\n",
    "### Distributions\n",
    "\n",
    "#### Discrete\n",
    "\n",
    "* Bernoulli\n",
    "* Uniform\n",
    "* Binomial/Multinomial\n",
    "* Negative Binomial\n",
    "* Geometric\n",
    "* Hypergeometric\n",
    "* Poisson\n",
    "    * Approximates binomial\n",
    "    * Thinning\n",
    "    * Poisson Process\n",
    "\n",
    "* Conditional Expectation for Discrete Variables:\n",
    "#### Continuous\n",
    "            \n",
    "* Uniform\n",
    "* Exponential\n",
    "    * Min/Max\n",
    "    * Competing\n",
    "* Beta\n",
    "* Gamma\n",
    "\n",
    "* Normal\n",
    "    * Approximates binomial +    \n",
    "    * Joint Distribution of 2 Independent Standard Normal Random Variables - \n",
    "* Rayleigh\n",
    "\n",
    "\n",
    "* Change of Variable (Continuous)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Inference\n",
    "\n",
    "* Observational data - data collected outside the context of an explicitly created experiment, or used outside the context of the original experiment\n",
    "* The problem with observational data\n",
    "    * Can't use it to prove causal relations\n",
    "    * Confounding variables/lurking variables/unobserved heterogeneity - unmeasured and uncontrolled attributes in the data\n",
    "        * It is impossible to clearly formulate any relationship as causal when unobserved heterogeneity is present - any correlation may be attributed to the existence of these confounding variables\n",
    "* Field experiments - randomized studies conducted in real-world settings\n",
    "* Naturally occurring experiments - interventions randomly assigned by some existing institution\n",
    "* Downstream experiments - intervention affects outcome of interest but also potentially other outcomes as well, which may also be studied\n",
    "* Quasi-experiments - near-random process cause subjects to receive different treatments, but not explicitly random (i.e. close elections, natural phenomenon like weather/disasters)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* For any/every treatment effect, there exists two potential outcomes:\n",
    "    * $Y_i(1)$ represents the outcome when the treatment effect is applied to subject $Y_i$\n",
    "    * $Y_i(0)$ represents the outcome when the treatment effect is not applied to subject $Y_i$\n",
    "    * In reality, we can only ever know one of these - when the treatment effect is/isn't applied, one potential outcome is realized and the other remains a hypothetical\n",
    "* $Y_i = d_i Y_i(1) +(1-d_i)Y_i(0)$, where $d_i$ is an indicator variable such that $d_i = 1$ when the ith subject has received the treatment and  $d_i = 0$ when the ith subject has not received the treatment \n",
    "    * $d_i$ is the treatment that is actually applied/not applied (variable), $D_i$ is the hypothetical treatment that has not yet been applied (random variable)\n",
    "        * $d_i$ is the realization of $D_i$\n",
    "    * $E[Y_i(1)|d_i=1]$ represents the expectation of $Y_i(1)$ when subject i is selected at random from the treated subjects\n",
    "    * $E[Y_i(1)|d_i=0]$ represents the expectation of $Y_i(1)$ when subject i is selected at random from the untreated subjects (which is a hypothetical quantity that is impossible to observe)\n",
    "* $\\tau_i$ represents the causal effect of the treatment, and is defined as the difference between the two potential outcomes \n",
    "    * $\\tau_i = Y_i(1) - Y_i(0)$\n",
    "    * $\\tau_i$ is a hypothetical quantity given that we can never really know both outcomes\n",
    "* Average Treatment Effect (ATE) is defined as the average of $\\tau_i$ for all $i$s\n",
    "    * $ATE=\\frac{1}{N} \\sum_i^N \\tau_i$\n",
    "    * In other terms, $ATE = E[Y_i(1)-Y_i(0)]$\n",
    "    * While different subjects will have different $\\tau_i$, the ATE describes how outcomes change on average when going from untreated to treated\n",
    "* Problem: we will never know both $Y_i(1)$ and $Y_i(0)$ \n",
    "* Solution: if we assign treatments to random subjects, the expectation of the treatment and control groups are identical. \n",
    "    * $E[Y_i(1)|D_i=1]=E[Y_i(1)]=E[Y_i(1)|D_i=0]$\n",
    "    * $E[Y_i(0)|D_i=0]=E[Y_i(0)]=E[Y_i(0)|D_i=1]$\n",
    "    * Treatment and control groups have same expected potential outcome\n",
    "    * $ATE = E[Y_i(1)-Y_i(0)] = 0$\n",
    "    * $ATE = E[Y_i(1)|D_i=1]-E[Y_i(0)|D_i=0]$\n",
    "        * Estimate ATE by taking difference between two sample means\n",
    "        * $E[Y_i(1)|D_i=1]-E[Y_i(0)|D_i=0] = E[Y_i(1)] - E[Y_i(0)] = E[\\tau_i] = ATE$\n",
    "        * When treatments are randomly assigned, comparison of average outcomes in treatment and control groups (difference-in-means estimator) is unbiased estimator of ATE\n",
    "* Selection problem - receiving treatment may be systemically related to potential outcomes (sets are not truly random)\n",
    "    * $E[Y_i(1)|D_i=1]-E[Y_i(0)|D_i=0] = $ expected difference between treated and untreated outcomes \n",
    "    * $E[Y_i(1)|D_i=1]-E[Y_i(0)|D_i=0] = E[Y_i(1)|D_i=1] + E[Y_i(0)|D_i=1] - E[Y_i(0)|D_i=0]$ \n",
    "        * $E[Y_i(1)|D_i=1] = $ ATE among the treated \n",
    "        * $E[Y_i(0)|D_i=1] - E[Y_i(0)|D_i=0] = $ selection bias term\n",
    "        * With random assignment/selection, selection bias term is 0 ($E[Y_i(0)|D_i=1] - E[Y_i(0)|D_i=0] = 0$), so the ATE among the treated is equal to the expected difference between treated and untreated outcomes\n",
    "        * Without random assignment/selection, the apparent treatment effect will be a mixture of selection bias and the ATE for a subset of subjects\n",
    "            * Therefore, random assignment is necessary to specifically identify ATE among the treated subjects\n",
    "* Excludability - that the potential outcome depends solely on whether or not the subject receives the treatment\n",
    "    * Treatment assignment $z_i$ should only affect $d_i$\n",
    "* Non-interference - the subject itself receives the treatment, not the treatment of other subjects\n",
    "\n",
    "\n",
    "\n",
    "* Standard Deviation - $\\sqrt{\\frac{1}{N} \\sum_1^N (X_i - \\bar X)^2}$ \n",
    "    * When X is a random sample from larger population containing N* subjects with an unknown mean: \n",
    "        * Standard Deviation - $\\sqrt{\\frac{1}{N-1} \\sum_1^N (X_i - \\bar X)^2}$ \n",
    "* Standard Error - $\\sqrt{\\frac{1}{J} \\sum_1^J (\\hat\\theta_j - \\bar{\\hat \\theta})^2}$ \n",
    "    * Standard error - standard deviation of a sampling distribution \n",
    "    * $J$ - number of possible ways of randomly assigning subjects\n",
    "    * $\\hat\\theta_j$ - estimate we get from the jth randomization\n",
    "    * $\\bar{\\hat \\theta}$ - average estimate of all j randomizations\n",
    "* Variance - variance of an observed or potential outcome for a set of N subjects is the average squared deviation from the mean for all N subjects\n",
    "    * Ex. $Var(Y_i(1)) = \\sqrt{\\frac{1}{N} \\sum_1^N (Y_i(1) - \\frac{\\sum_1^N (Y_i(1))}{N})^2} $\n",
    "    * Higher variance = higher dispersion around the mean\n",
    "    * Variance of 0 means the variable is a constant\n",
    "* Covariance - covariance between two variables = subtract the mean from each and calculate the average cross-product of the result:\n",
    "    * Ex. $Cov(Y_i(0),Y_i(1)) = \\sqrt{\\frac{1}{N} \\sum_1^N (Y_i(0) - \\frac{\\sum_1^N (Y_i(0))}{N})(Y_i(1) - \\frac{\\sum_1^N (Y_i(1))}{N})} $\n",
    "    * Covariance is measure of association between two variables\n",
    "    * Negative covariance implies low values of one coincides with higher values of another (inverse relationship)\n",
    "    * Positive covariance implies high values of one coincides with high values of another\n",
    "* Standard Error of estimated ATE - $SE(\\hat{ATE}) = \\sqrt{\\frac{1}{N-1} [\\frac{m Var(Y_i(0))}{N-m} + \\frac{(N-m) Var(Y_i(1))}{m} + 2Cov(Y_i(0),Y_i(1))]}$\n",
    "    * N observations\n",
    "    * m treated units\n",
    "    * Implications for experiment design\n",
    "        * Larger N = smaller standard error\n",
    "        * Smaller variance (of either $Y_i(0),Y_i(1)$) = smaller SE\n",
    "        * Smaller covariance (of $Y_i(0),Y_i(1)$) = smaller SE\n",
    "        * Similar variances (of $Y_i(0),Y_i(1)$) => we should assign equal number of subjects to control and treatment groups \n",
    "            * If different variance, assign more subjects to the group with higher variance\n",
    "* True SE is unknown\n",
    "    * We don't know the covariance between treatment/control - if we did, no reason to run the experiment\n",
    "    * Need to estimate SE\n",
    "    * Formula for estimating SE of ATE in practice: $\\sqrt{\\frac{\\hat{Var}(Y_i(0))}{N-m} + \\frac{\\hat{Var}(Y_i(1))}{m}}$\n",
    "        * $\\hat{Var}(Y_i(1)) = \\frac{1}{m-1} \\sum_{1}^{m} (Y_i(1)|d_i = 1 - \\frac{\\sum_1^m Y_i(1)|d_i = 1}{m})^2$\n",
    "        * $\\hat{Var}(Y_i(0)) = \\frac{1}{N-m-1} \\sum_{N}^{m+1} (Y_i(0)|d_i = 0 - \\frac{\\sum_{m+1}^N Y_i(0)|d_i = 0}{N-m})^2$\n",
    "        * This is the standard approach, which is to use a conservative estimation formula that is at least as large as the theoretical equation for the SE of the estimate ATE ($SE(\\hat{ATE}) = \\sqrt{\\frac{1}{N-1} [\\frac{m Var(Y_i(0))}{N-m} + \\frac{(N-m) Var(Y_i(1))}{m} + 2Cov(Y_i(0),Y_i(1))]}$)\n",
    "            * Conservative formula assumes treatment effect is the same for all subjects (correlation between $Y_i(0),Y_i(1)$ is 1)\n",
    "        * Calculating sample variances, divide by $n-1$ to account for the fact that 1 observation is expended when we calculate the sample mean\n",
    "* One-tailed hypothesis - whether the treatment results in a change in one direction (choose either greater or less than)\n",
    "* Two-tailed hypothesis - whether the treatment results in a change in either direction (either greater or less than)\n",
    "* For large N, number of random assignments becomes large\n",
    "    * For $N=50$ and treatment/control groups of equal size, number of possible randomizations = $\\frac{50!}{25! 25!}$\n",
    "        * Approximate sampling distribution by randomly sampling from set of all possible random assignments\n",
    "* Randomization inference - calculation of p-values based on sets of possible randomizations\n",
    "* Sharp null hypothesis - treatment effect is 0 for all observations\n",
    "    * If true, $Y_i(1)=Y_i(0)$, assume all $\\tau_i = 0$\n",
    "    * Treatment is no different than control\n",
    "    \n",
    "    \n",
    "* Statistical power ingredients:\n",
    "    * Sample size\n",
    "    * Effect size\n",
    "    * Population variance (in respect to the effect)\n",
    "\n",
    "* Chi-Square ($\\chi^2$) test:\n",
    "    * Hypothesis testing method. Two common Chi-square tests involve checking if observed frequencies in one or more categories match expected frequencies\n",
    "    * Chi-square formula: $\\chi^2_c = \\sum \\frac{(O_i - E_i)^2}{E_i}$\n",
    "        * c = degrees of freedom\n",
    "        * O = observed value\n",
    "        * E = expected value\n",
    "    * Low value = high correlation between two sets of data (if two sets were equal then $\\chi^2_c = 0 $), high value = low correlation\n",
    "    * Can only be used on raw numerical values! Not percentages/proportions/means\n",
    "    * Goodness of Fit Test: if a sample matches a population\n",
    "        * One variable\n",
    "        * Degrees of freedom = Number of categories minus 1\n",
    "    * Test of independence: if the distributions of categorical variables differ from each other\n",
    "        * Two variables\n",
    "        * Degrees of freedom = Number of categories for first variable minus 1, multiplied by number of categories for second variable minus 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning \n",
    "\n",
    "### Dimensionality\n",
    "* Dimensionality\n",
    "    * Principle Components Analysis\n",
    "    * Canonical Components Analysis\n",
    "    \n",
    "### Classification versus Regression\n",
    "* Classification - used to separate data into classes/categories\n",
    "    * Discriminant Analysis\n",
    "    * Naive Bayes\n",
    "    * Logistic Regression\n",
    "    * K-Nearest Neighbors\n",
    "    * \n",
    "* Regression - used to model relationships within the data\n",
    "    * Linear Regression\n",
    "    *\n",
    "\n",
    "### Supervised Learning versus Unsupervised Learning\n",
    "* Supervised Learning - program is trained on labeled data \n",
    "    * Regression\n",
    "        * Linear Regression\n",
    "        * Multiple Regression\n",
    "        * Polynomial Regression\n",
    "        * Logistic Regression\n",
    "    * Discriminant Analysis\n",
    "    * Perceptron\n",
    "    * Naive Bayes\n",
    "    * Decision Trees\n",
    "* Unsupervised Learning - program is trained on unlabeled data, aims to find patterns in data by itself (i.e. clustering)\n",
    "    * Expectation Maximization\n",
    "    * K-Nearest Neighbors\n",
    "    \n",
    "    \n",
    "### Parametric Modeling versus Unparametric Modeling\n",
    "* Parametric Modeling - assumes the data follows some known distribution that we can estimate (number of parameters are fixed according to the sample size)\n",
    "    * Logistic Regression\n",
    "    * Linear Discriminant Analysis\n",
    "    * Perceptron\n",
    "    * Naive Bayes\n",
    "    * Simple Neural Networks\n",
    "* Non-Parametric Modeling - does not assume the data follows some known distribution, no reliance on distribution assumptions (numvber param)\n",
    "    * K-Nearest Neighbors\n",
    "    * Decision Trees\n",
    "    * Support Vector Machines\n",
    "    \n",
    "### Generative Models versus Nongenerative/Discriminative Models\n",
    "* Generative Model - model can be used to generate new data after analyzing existing data\n",
    "    * Discriminant Analysis\n",
    "    * Naive Bayes\n",
    "    * K-Nearest Neighbors\n",
    "    \n",
    "* Nongenerative/Discriminative - model cannot be used to generate new data, only used to classify\n",
    "    * Regression\n",
    "    * Simple Neural Networks\n",
    "    * Support Vector Machines\n",
    "    * Decision Trees\n",
    "\n",
    "\n",
    "* Reinforcement Learning - semi-supervised, given labels for some data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neural Net\n",
    "\n",
    "#### The Perceptron\n",
    "The perceptron is a simple algorithm for linear classification:\n",
    "\n",
    "\n",
    "$f(x) = 1 $ when $ w*x + b > 0$, else $f(x) = 0$ \n",
    "\n",
    "\n",
    "An input $x$ is classified as class $1$ if it falls on one side of the line of $ w*x + b > 0$, and $0$ if it falls on the other.\n",
    "\n",
    "\n",
    "In the case of multiclass decisionmaking, we can adjust the perceptron by using a weight vector for each class: weight $w_y$ for class $y$.  The scoring function for a class y now becomes $w_y * f(x)$, and we use the highest score to determine the class:\n",
    "\n",
    "\n",
    "$y = argmax_y [w_y * f(x)]$\n",
    "\n",
    "\n",
    "To decide what exactly these decision boundaries should look like, the perceptron can be trained to find optimal weights using the following steps:\n",
    "\n",
    "\n",
    "1. Initialize the weights to some random value or 0. \n",
    "2. For each data point $i$ in the training set $X$:\n",
    "\n",
    "\n",
    "Calculate the score by $y_i(t) = f[w(t)*x_i] = f[w_0(t)x_{i,0}+w_1(t)x_{i,1}...+w_n(t)x_{i,n}]$, where $t$ is the time of this step. \n",
    "        \n",
    "        \n",
    "Update weights: $w_j(t+1) = w_j(t) + r*(d_i - y_i(t))x_{i,j}$ where r is a preset learning rate and d_i is the correct label of that input.\n",
    " \n",
    "\n",
    "\n",
    "We repeat these steps until either all inputs are classified correctly or we reach some predetermined accuracy. Step 2b can be seen as adjusting the weight according to whether or not the algorithm correctly classifies the input.  If the actual label is equal to the generated label, then $d_i = y_i(t)$ so $w_j(t+1) = w_j(t) + 0$.  If the labels differ, then we adjust the weight according to the learning rate.\n",
    "\n",
    "\n",
    "If we want to output probabilities instead of just the label, we can adjust the perceptron by using the softmax function: \n",
    "\n",
    "\n",
    "$softmax(x_i) = \\sigma(x_i) = \\frac{e^{f(x_i)^T w_j}}{\\sum_{k=1}^N e^{f(x_i)^T w_k}} = P(y_i = j|x_i)$, where $i$ is the ith data point and $j$ is jth class\n",
    "\n",
    "\n",
    "Obviously, our goal is to find the values of the weight vector that would maximize these probabilities.  If the log likelihood turns out to be differentiable, then we can use this to determine some sort of loss function for optimization.\n",
    "\n",
    "The softmax normalizes the vector output by the function $f$ to produce probabilities that sum to 1.  The log likelihood of a particular set of weights can be expressed as:\n",
    "\n",
    "$ll(w) = log = \\prod_{i=1}^m P(y_i | x_i;w) = \\sum_{i=1}^m log P(y_i | x_i;w)$\n",
    "\n",
    "The multilayer perceptron, where each layer of perceptrons takes as the output of previous layer of perceptron(s) as input, is a Universal Function Approximator, meaning that a two-layer neural network with a sufficient number of neurons can approximate any continuous function to any desired accuracy.\n",
    "\n",
    "#### Activation Functions\n",
    "\n",
    "A basic neural network is a layered network of perceptrons, similar to the multilayer perceptron, but with an added nonlinearity function applied to the output of every individual perceptron.  Common nonlinearities are:\n",
    "\n",
    "* ReLU (Rectified Linear Unit): \n",
    "\n",
    "    $$ f(x)=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0 & x<0 \\\\\n",
    "      x & x \\geq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    "The ReLU is useful for several reasons. It can reduce computational complexity by reducing activations, and reducing vanishing/exploding gradient problems.\n",
    "\n",
    "The vanishing gradient problem naturally results from gradient-based optimization.  Each weight in a neural network receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training.  This can result in a vanishingly small gradient that doesn't meaningfully change the weights, leading to the overall network becoming stuck during training.\n",
    "\n",
    "Similarly, the exploding gradient problem is when constant scaling of weights leads to obscenely huge values that can lead to memory or computational issues during training.\n",
    "\n",
    "The basic ReLU, however, does have its own potential issues.  The dying ReLU problem is when neurons are put into 'dead states' by a large number of weights going to 0.  This can happen when the learning rate is set too high.\n",
    "\n",
    "* Leaky ReLU:\n",
    "\n",
    "    $$ f(x)=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      ax & x<0 \\\\\n",
    "      x & x \\geq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    "Leaky ReLU is based on ReLU, but has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training. This type of activation function can solve the dying ReLU problem and is popular in tasks that might involve sparse gradients, like when training generative adversarial networks.\n",
    "\n",
    "* Sigmoid: \n",
    "\n",
    "    $$ f(x)= \\frac{1}{1+e^{-x}}$$\n",
    "    \n",
    "\n",
    "#### Gradient Descent \n",
    "\n",
    "To work with these perceptron layers, we have to be able to algorithmically optimize the weights in some way. Specifically, we want to maximize the log likelihood probability function of the weights.  We can differentiate the log likelihood function to find the gradient vector consisting of its partial derivatives for each parameter: $\\triangledown_w ll(w) = [\\frac{\\partial ll(w)}{\\partial w_1}, ..., \\frac{\\partial ll(w)}{\\partial w_n}]$\n",
    "\n",
    "This gradient vector gives the local direction of steepest ascent (or descent if we reverse the vector). Gradient ascent is a greedy algorithm that calculates this gradient for the current values of the weight parameters,\n",
    "then updates the parameters along the direction of the gradient, scaled by a step size, $\\alpha$. Specifically the algorithm looks as follows:\n",
    "\n",
    "1. Initialize weights $[w_1,...,w_n]$\n",
    "\n",
    "2. For $i = 0, 1, 2...$, $w = w + \\alpha \\triangledown_w ll(w)$\n",
    "\n",
    "\n",
    "If we want to minimize a function $f$ instead of $w$, the update should subtract the scaled gradient $w = w + \\alpha \\triangledown_w f(w)$.  This gives us the gradient descent algorithm.\n",
    "\n",
    "Now that we have a method for computing gradients for all parameters of the network, we can use gradient descent methods to optimize the parameters to get high accuracy on our training data. For example, suppose we have designed some classification network to output probabilities of classes $y$ for data points $x$, and have $m$ different training datapoints (see the Measuring Accuracy section for more on this). Let $w$ be all the\n",
    "parameters of our network. We want to find values for the parameters $w$ that maximize the likelihood of the true class probabilities for our data, so we have the following function to run gradient ascent on:\n",
    "\n",
    "$$ll(w) = log \\prod_{i=1}^n P(y_i | x_i ;w) = \\sum log P(y_i | x_i;w)$$\n",
    "\n",
    "where $x_i,...x_n$ are the n datapoints in the training set.  One way to try to minimize this function is, at each iteration of gradient descent, to use all the data points $x_1,...,x_m$\n",
    "\n",
    "to compute gradients for the parameters w, update the parameters, and repeat until the parameters converge (at which point we’ve reached a local minimum of the function).\n",
    "This technique, known as batch gradient descent, is rarely done in practice, since datasets are typically large enough that computing gradients for this full likelihood function will be very slow. Instead, we’ll typically use mini-batching. Mini-batching rotates through randomly sampled batches of $k$ data points at a time, taking one batch for each step of gradient descent and computing gradients of the loss function using\n",
    "only that batch (so that the sum above is over the $k$ datapoints in the batch, rather than all $m$ datapoints in the training set). This allows us to compute each gradient update much more quickly, and often still makes fast progress toward the minimum of the function. The limit where the batch size $k = 1$ is known as stochastic gradient descent (SGD). In SGD, we randomly sample a single example from the training dataset at each step of gradient descent, compute parameter gradients using the network’s loss on that single\n",
    "example, update the parameters, and repeat (sampling another example from the training set). \n",
    "\n",
    "\n",
    "#### Backpropagation\n",
    "\n",
    "To efficiently calculate the gradients for each parameter in a neural network, we will use an algorithm known as backpropagation. Backpropagation represents the neural network as a dependency graph of operators and operands, called a computational graph. The graph structure allows us to efficiently compute both the network’s error (loss) on input data, as well as the gradients of each parameter with respect to the loss. These gradients can be used in gradient descent to adjust the network’s parameters and minimize the loss on the training data.\n",
    "\n",
    "##### The Chain Rule\n",
    "\n",
    "The chain rule is the fundamental rule from calculus which both motivates the usage of computation graphs and allows for a computationally feasible backpropagation algorithm. Mathematically, it states that for a variable $z$ which is a function of $n$ variables $x_1,..., x_n$ and each $x_i$ is a function of $m$ variables $t_1,...,t_m$, then\n",
    "we can compute the derivative of $z$ with respect to any $t_i$ as follows:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial t_i} = \\frac{\\partial f}{\\partial x_1}\\frac{\\partial x_1}{\\partial t_i} + \\frac{\\partial f}{\\partial x_2}\\frac{\\partial x_2}{\\partial t_i}+...+\\frac{\\partial f}{\\partial x_n}\\frac{\\partial x_n}{\\partial t_i}$$\n",
    "\n",
    "In the context of computation graphs, this means that to compute the gradient of a given node $t_i$ with respect to the output $z$, we take a sum of children ($t_i$) terms.\n",
    "\n",
    "\n",
    "The forward pass is when we calculate the value of a node $k$ by applying each node’s operation to its input values coming from its parent nodes until we arrive at node $k$.\n",
    "\n",
    "The backward pass is when we calculate the gradients of the function computed by the graph: the value after each node is the partial derivative of the last node f value with respect to the variable at that node.  The backward pass computes gradients by starting at the final node (which has a gradient of 1 since $\\frac{\\partial f}{\\partial f} = 1$ and passing and updating gradients backward through the graph. Intuitively, each node’s gradient measures how much a change in that node’s value contributes to a change in the final node’s value. This will be the product of how much the node contributes to a change in its child node, with how much the child node contributes to a change in the final node. Each node receives and combines gradients from its children, updates this combined gradient based on the node’s inputs and the node’s operation, and then passes the updated\n",
    "gradient backward to its parents. Computation graphs are a great way to visualize repeated application of the chain rule from calculus, as this process is required for backpropagation in neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
