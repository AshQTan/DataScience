{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Glossary\n",
    "\n",
    "## Statistics\n",
    "\n",
    "### Foundation\n",
    "\n",
    "\n",
    "* Chance - proportion of times event should happen over repeated trials\n",
    "* P(A) - proportion of times event A happens in n trials\n",
    "* Law of Large Numbers - as n, the number of trials, grows larger and approaches infinity,  P(A) approaches \n",
    "* Outcome space - all possible results of a trial, P(outcome space) = 1  for any trial (i.e. the probability of a quarter flipping either heads or tails up is 100%).  The probability of an impossible result is 0.\n",
    "* Complement of an event A ($A_c$ or $\\overline{\\mbox{A}}$, depending on notation) - all events not including A, with total probability of $1 - P(A)$\n",
    "* Union of events A and B ($A \\cup B$) - any event that includes event A, event B, or both A and B.\n",
    "* Intersection of events A and B ($A \\cap B$) - any event that includes both A and B\n",
    "* Subset: An event A is a subset of event B if the A is within B\n",
    "* Partition - an event can be partitioned into non-intersecting sub-events (event A partitioned into sub-events $A_1, A_2...A_n)$.  If the probability of the intersection of sub-events is 0 (no sub-events overlap) then $A_1, A_2...A_n)$ form a partition.\n",
    "    * $A_1, A_2, A_3... A_n$ form a partition of $A$ if $A = A_1 \\cup A_2 \\cup A_3 ...\\cup A_n$ and $A_i \\cap A_j$ for all $i \\neq j, i,j \\leq n$ \n",
    "    \n",
    "    \n",
    "    \n",
    "* Rules of Probability:\n",
    "    * Probability of any event within the outcome space is at least 0.  $(P(A) â‰¥ 0)$\n",
    "    * If the sub-events $A_1, A_2...A_n$ form a partition of the event $A$, then $P(A) = P(A_1) + P(A_2) + ... + P(A_n)$\n",
    "    * Probability of the outcome space is 1; the sum of the probability of all possible and impossible events within the outcome space is 1.\n",
    "* Probability Space - defines the universe of a statistical model using 3 parts:\n",
    "    * Sample space -  the collection of all possible outcomes\n",
    "    * Event space - the collection of all possible sets of possible outcomes.\n",
    "    * Probability measure - a function that maps each event to a probability within the $[0,1]$ interval\n",
    "* Independence - event A is independent of event B if the occurrence of one does not affect the other\n",
    "* Multiplicative Law of Probability - if A and B are independent events, then $P(A \\cap B) = P(A)*P(B)$\n",
    "* Addition Law of Probability - if A and B are independent events, then $P(A \\cup B) = P(A)+P(B)-P(A \\cap B)$\n",
    "* Conditional Probability ($P(A|B)$) - reframing the probability of an event A given information about the occurrence of some event B\n",
    "* Law of Total Probability - for a partition $A_1, A_2...A_3$ of the sample space and for event $B$ of the sample space, $P(B) = \\sum_i P(B \\cap A_i)$\n",
    "    * If each partition $A_i$ has a positive probability (i.e. the subevent $A_i$ has a non-zero probability of existing), then by the Multiplicative Law of Probability, $P(B) = \\sum_i P(B|A_i)P(A_i)$\n",
    "* Bayes' Rule:\n",
    "    * $P(B|A) = \\frac{P(A|B)}{P(A)}$\n",
    "    * Alternatively, $P(B|A)= \\frac{P(A|B)P(B)}{\\sum_i P(A|B_i)P(B_i)}$\n",
    "\n",
    "\n",
    "* Random Variable - a variable without a fixed value.  Instead, a random variable describes any number of potential outcomes that may come from a random phenomenon.\n",
    "* Indicator random variable - a binary random variable used to describe failure/success (takes on value 0 or 1).  Often used in problems to simplify calculations.\n",
    " \n",
    "* Distribution - a function that divides the probability of outcome space into subsets in such a way that satisfies the rules of probability. \n",
    "    * For example, the distribution of a coin toss is $P(Heads) = 0.5, P(Tails = 0.5)$.\n",
    "    * Discrete distributions - random variables take on integer values (i.e. dice rolls, number of tickets bought in an hour, etc.)\n",
    "    * Continuous distributions - random variables take on continuous (decimal/real) values (i.e. time, distance, etc.)\n",
    "* Probability Mass Function/Probability Density Function (PMF/PDF) - a function that describes the probability of a random variable taking on a certain value (PMF/PDF: $P(X=x) = f(x)$)\n",
    "    * Probability Mass Function - used for discrete distributions\n",
    "    * Probability Density Function - used for continuous distributions.  A little more complicated than PMFs, since the absolute probability of a random value equaling an exact value is 0 due to the issue of preciseness (0 vs 0.00000000001).  Instead, the PDF describes a relative probability of a random value being within a certain interval containing that exact value.\n",
    "* Cumulative Density Function (CMF/CDF) - a function that describes the probability of a random variable being less than or equal to a certain value(PMF/PDF: $P(X<x) = F(x)$)\n",
    "    * For discrete distributions, the CDF can be defined as $P(X<x) = F(x) = \\sum_{x_i<x} f(X=x_i)$.  Essentially, we are adding the all the probabilities of X taking on all the values that are less than x.\n",
    "    * For continuous distributions, the CDF can be defined as  $P(X<x) = F(x) = \\int_{-\\infty}^x f(X=x_i)dx$.  Essentially, we are integrating to find the area under the curve up to the point of x.\n",
    "\n",
    "    \n",
    "* Joint Distributions\n",
    "* Marginal Distributions\n",
    "* Conditional Distribution\n",
    "\n",
    "\n",
    "* Expected Value - essentially a weighted sum of all outcomes by their probabilities\n",
    "    * For discrete distributions, $E(X) = \\sum P(x)x$\n",
    "    * For continuous distributions, $E(X) = \\int_{-\\infty}^{\\infty} P(x)x dx$\n",
    "* Law of the Unthinking Statistician (LOTUS) - used to calculate the expectation of a function of a random variable.\n",
    "    * LOTUS (discrete): $E(g(X)) = \\sum g(x)f(x)$, where $f(x)$ is the PMF \n",
    "    * LOTUS(continuous): $E(g(X)) =\\int_{-\\infty}^{\\infty} f(x)g(x) dx$, where $f(x)$ is the PDF \n",
    "    \n",
    "* Variance\n",
    "* Covariance\n",
    "* Correlation\n",
    "\n",
    "\n",
    "* Markov's Inequality\n",
    "    * Deals with edge cases, not non-negative\n",
    "* Chebyshev's Inequality\n",
    "    * Cbebyshev: sets bounds on both sides, allows better centering\n",
    "* Central Limit Theorem\n",
    "* Law of Averages\n",
    "\n",
    "\n",
    "* Craps Principle - for independent trials each with probability $p$ of success:\n",
    "    1. Find event A of your choosing with $P(A) = \\frac{1}{2}$\n",
    "    2. Wait until Success + Failure or Failure + Success; if Success + Success or Failure + Failure, try again\n",
    "    3. Find event B with p of your choosing\n",
    "    \n",
    "\n",
    "### Distributions\n",
    "\n",
    "#### Discrete\n",
    "\n",
    "* Bernoulli\n",
    "* Uniform\n",
    "* Binomial/Multinomial\n",
    "* Negative Binomial\n",
    "* Geometric\n",
    "* Hypergeometric\n",
    "* Poisson\n",
    "    * Approximates binomial\n",
    "    * Thinning\n",
    "    * Poisson Process\n",
    "\n",
    "* Conditional Expectation for Discrete Variables:\n",
    "#### Continuous\n",
    "            \n",
    "* Uniform\n",
    "* Exponential\n",
    "    * Min/Max\n",
    "    * Competing\n",
    "* Beta\n",
    "* Gamma\n",
    "\n",
    "* Normal\n",
    "    * Approximates binomial +    \n",
    "    * Joint Distribution of 2 Independent Standard Normal Random Variables - \n",
    "* Rayleigh\n",
    "\n",
    "\n",
    "* Change of Variable (Continuous)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning \n",
    "\n",
    "### Dimensionality\n",
    "* Dimensionality\n",
    "    * Principle Components Analysis\n",
    "    * Canonical Components Analysis\n",
    "    \n",
    "### Classification versus Regression\n",
    "* Classification - used to separate data into classes/categories\n",
    "    * Discriminant Analysis\n",
    "    * Naive Bayes\n",
    "    * Logistic Regression\n",
    "    * K-Nearest Neighbors\n",
    "    * \n",
    "* Regression - used to model relationships within the data\n",
    "    * Linear Regression\n",
    "    *\n",
    "\n",
    "### Supervised Learning versus Unsupervised Learning\n",
    "* Supervised Learning - program is trained on labeled data \n",
    "    * Regression\n",
    "        * Linear Regression\n",
    "        * Multiple Regression\n",
    "        * Polynomial Regression\n",
    "        * Logistic Regression\n",
    "    * Discriminant Analysis\n",
    "    * Perceptron\n",
    "    * Naive Bayes\n",
    "    * Decision Trees\n",
    "* Unsupervised Learning - program is trained on unlabeled data, aims to find patterns in data by itself (i.e. clustering)\n",
    "    * Expectation Maximization\n",
    "    * K-Nearest Neighbors\n",
    "    \n",
    "    \n",
    "### Parametric Modeling versus Unparametric Modeling\n",
    "* Parametric Modeling - assumes the data follows some known distribution that we can estimate (number of parameters are fixed according to the sample size)\n",
    "    * Logistic Regression\n",
    "    * Linear Discriminant Analysis\n",
    "    * Perceptron\n",
    "    * Naive Bayes\n",
    "    * Simple Neural Networks\n",
    "* Non-Parametric Modeling - does not assume the data follows some known distribution, no reliance on distribution assumptions (numvber param)\n",
    "    * K-Nearest Neighbors\n",
    "    * Decision Trees\n",
    "    * Support Vector Machines\n",
    "    \n",
    "### Generative Models versus Nongenerative/Discriminative Models\n",
    "* Generative Model - model can be used to generate new data after analyzing existing data\n",
    "    * Discriminant Analysis\n",
    "    * Naive Bayes\n",
    "    * K-Nearest Neighbors\n",
    "    \n",
    "* Nongenerative/Discriminative - model cannot be used to generate new data, only used to classify\n",
    "    * Regression\n",
    "    * Simple Neural Networks\n",
    "    * Support Vector Machines\n",
    "    * Decision Trees\n",
    "\n",
    "\n",
    "* Reinforcement Learning - semi-supervised, given labels for some data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
